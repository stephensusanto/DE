{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6260369a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (4.13.5)\n",
      "Requirement already satisfied: lxml in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (5.2.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: selenium in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (4.35.0)\n",
      "Requirement already satisfied: pyperclip in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (1.10.0)\n",
      "Requirement already satisfied: googlemaps in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (4.10.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (2.3.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (1.7.2)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 18)) (3.10.6)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 19)) (0.13.2)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (0.21.0)\n",
      "Requirement already satisfied: duckdb in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 25)) (1.4.0)\n",
      "Requirement already satisfied: uuid in /opt/anaconda3/lib/python3.12/site-packages (from -r requirements.txt (line 28)) (1.30)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->-r requirements.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->-r requirements.txt (line 2)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->-r requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->-r requirements.txt (line 2)) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4->-r requirements.txt (line 3)) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4->-r requirements.txt (line 3)) (4.14.1)\n",
      "Requirement already satisfied: trio~=0.30.0 in /opt/anaconda3/lib/python3.12/site-packages (from selenium->-r requirements.txt (line 8)) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in /opt/anaconda3/lib/python3.12/site-packages (from selenium->-r requirements.txt (line 8)) (0.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from selenium->-r requirements.txt (line 8)) (1.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 15)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 15)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->-r requirements.txt (line 15)) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 17)) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 17)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->-r requirements.txt (line 17)) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 18)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 18)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 18)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 18)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 18)) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 18)) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->-r requirements.txt (line 18)) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 15)) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.30.0->selenium->-r requirements.txt (line 8)) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.30.0->selenium->-r requirements.txt (line 8)) (2.4.0)\n",
      "Requirement already satisfied: outcome in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.30.0->selenium->-r requirements.txt (line 8)) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.30.0->selenium->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/anaconda3/lib/python3.12/site-packages (from trio-websocket~=0.12.2->selenium->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium->-r requirements.txt (line 8)) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium->-r requirements.txt (line 8)) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c416354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import mimetypes\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import googlemaps\n",
    "import uuid\n",
    "\n",
    "# selenium method\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pyperclip\n",
    "\n",
    "import time\n",
    "from datetime import date, datetime\n",
    "\n",
    "# analysis\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# load environment file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# duckdb\n",
    "import duckdb\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdd4b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadFile:\n",
    "    \"\"\"Utility class for downloading files with progress and CER/ABS support.\"\"\"\n",
    "\n",
    "    def status_bar_api(self, api_url: str, csv_filename: str = None, status:str = None):\n",
    "        \"\"\"\n",
    "        Download a file from API URL with progress bar.\n",
    "\n",
    "        Args:\n",
    "            api_url: Direct URL to download.\n",
    "            csv_filename: Optional filename; if None, determined from URL or headers.\n",
    "        \"\"\"\n",
    "\n",
    "        if status == 'api':\n",
    "            with requests.get(api_url, stream=True) as response:\n",
    "                response.raise_for_status()\n",
    "                total_size = int(response.headers.get('content-length', 0))\n",
    "                chunk_size = 1024 * 1024  # 1 MB chunks\n",
    "                \n",
    "                chunks = []\n",
    "                with tqdm(total=total_size, unit='B', unit_scale=True, desc='Downloading') as pbar:\n",
    "                    for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                        if chunk:\n",
    "                            chunks.append(chunk)\n",
    "                            pbar.update(len(chunk))\n",
    "                \n",
    "                # Combine chunks into a single bytes object\n",
    "                content = b''.join(chunks)\n",
    "            \n",
    "            # Save CSV if filename is provided\n",
    "            if csv_filename:\n",
    "                with open(csv_filename, 'wb') as f:\n",
    "                    f.write(content)\n",
    "            print(f\"Download completed: {csv_filename}\")\n",
    "            return csv_filename\n",
    "        else:\n",
    "\n",
    "            response = requests.get(api_url, stream=True)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Determine filename\n",
    "            filename = csv_filename or os.path.basename(api_url)\n",
    "            if \"Content-Disposition\" in response.headers:\n",
    "                content_disposition = response.headers[\"Content-Disposition\"]\n",
    "                filename = content_disposition.split(\"filename=\")[-1].strip('\"')\n",
    "\n",
    "            # Guess extension if missing\n",
    "            if \".\" not in filename:\n",
    "                content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "                ext = mimetypes.guess_extension(content_type.split(\";\")[0].strip())\n",
    "                if ext:\n",
    "                    filename += ext\n",
    "\n",
    "            total_size = int(response.headers.get(\"content-length\", 0))\n",
    "            chunk_size = 8192  # 8 KB\n",
    "\n",
    "            with open(filename, \"wb\") as f, tqdm(\n",
    "                total=total_size, unit=\"B\", unit_scale=True, desc=filename\n",
    "            ) as progress_bar:\n",
    "                for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "\n",
    "            print(f\"Download completed: {filename}\")\n",
    "            return filename\n",
    "\n",
    "\n",
    "    def download_from_cer(self, url: str):\n",
    "        \"\"\"\n",
    "        Extract API URL from CER datasets using Selenium and download CSV.\n",
    "\n",
    "        Args:\n",
    "            url: CER dataset page URL.\n",
    "        \"\"\"\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "        # Find and click \"Copy API URL\" button\n",
    "        button = wait.until(\n",
    "            EC.element_to_be_clickable(\n",
    "                (By.XPATH, \"//button[.//span[contains(text(), 'Copy API URL')]]\")\n",
    "            )\n",
    "        )\n",
    "        button.click()\n",
    "        time.sleep(1)  # wait for clipboard update\n",
    "\n",
    "        api_url = pyperclip.paste()\n",
    "        driver.quit()\n",
    "\n",
    "        cer_code = url.split(\"/\")[-1]\n",
    "        api_url = (api_url.replace(\"ODataDataset\", \"Dataset\")).split(\"/\")[:-1]\n",
    "        api_url = \"/\".join(api_url)+f\"/{cer_code}.csv\"\n",
    "        print(\"Downloading from:\", api_url)\n",
    "\n",
    "        csv_filename = f\"{cer_code}.csv\"\n",
    "        return self.status_bar_api(api_url=api_url, csv_filename=csv_filename, status=\"api\")\n",
    "\n",
    "    def download_cer_markets(self, url: str):\n",
    "        \"\"\"\n",
    "        Download CER markets CSV for power stations/projects.\n",
    "\n",
    "        Args:\n",
    "            url: CER markets page URL.\n",
    "        \"\"\"\n",
    "        url_header = url.split('/')[2]\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find all divs containing file links\n",
    "        for div in soup.find_all(\"div\", class_=\"cer-accordion__body__item\"):\n",
    "            a_tag = div.find(\"a\", href=True)\n",
    "            if a_tag:\n",
    "                text = a_tag.get_text(strip=True).lower()\n",
    "                if \"csv\" in text and \"power stations\" in text and \"projects\" in text:\n",
    "                    file_href = a_tag[\"href\"]\n",
    "                    full_url = f\"https://{url_header}{file_href}\"\n",
    "                    self.status_bar_api(api_url=full_url, status=\"file\")\n",
    "\n",
    "    def download_abs(self, url: str, target_text=\"Economy and industry\"):\n",
    "        \"\"\"\n",
    "        Download ABS data CSV based on section name.\n",
    "\n",
    "        Args:\n",
    "            url: ABS Data by Regions page URL.\n",
    "            target_text: Section header to look for (default \"Economy and industry\").\n",
    "        \"\"\"\n",
    "        url_header = url.split('/')[2]\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        target_div = None\n",
    "        for div in soup.find_all(\"div\", class_=\"file-description-link-formatter\"):\n",
    "            h4 = div.find(\"h4\")\n",
    "            if h4 and target_text in h4.text:\n",
    "                target_div = div\n",
    "                break\n",
    "\n",
    "        if target_div:\n",
    "            a_tag = target_div.find(\"a\", href=True)\n",
    "            download_url = f\"https://{url_header}{a_tag['href']}\"\n",
    "            print(\"Found download URL:\", download_url)\n",
    "            self.status_bar_api(api_url=download_url, status=\"file\")        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513799fb",
   "metadata": {},
   "source": [
    "<h1>Retrieve Data Set </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f48d9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_data_set():\n",
    "    download = DownloadFile()\n",
    "    # first dataset\n",
    "    print(\"Processing Download First Dataset ....\")\n",
    "    cer_url = \"https://data.cer.gov.au/datasets/NGER/ID0243\"\n",
    "    download.download_from_cer(url=cer_url)\n",
    "    # second dataset\n",
    "    print(\"Processing Download Second Dataset ....\")\n",
    "    cer_markets_url = \"https://cer.gov.au/markets/reports-and-data/large-scale-renewable-energy-data\"\n",
    "    download.download_cer_markets(url=cer_markets_url)\n",
    "    # third dataset\n",
    "    print(\"Processing Download Third Dataset ....\")\n",
    "    abs_url = \"https://www.abs.gov.au/methodologies/data-region-methodology/2011-24#data-downloads\"\n",
    "    download.download_abs(url=abs_url)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff5598da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Download First Dataset ....\n",
      "Downloading from: https://api.cer.gov.au/datahub-public/v1/api/Dataset/NGER/dataset/ID0243.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 83.3k/83.3k [00:00<00:00, 56.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed: ID0243.csv\n",
      "Processing Download Second Dataset ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "power-stations-and-projects-accredited.csv: 100%|██████████| 25.8k/25.8k [00:00<00:00, 15.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed: power-stations-and-projects-accredited.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "power-stations-and-projects-committed.csv: 100%|██████████| 1.89k/1.89k [00:00<00:00, 15.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed: power-stations-and-projects-committed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "power-stations-and-projects-probable.csv: 100%|██████████| 2.22k/2.22k [00:00<00:00, 10.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed: power-stations-and-projects-probable.csv\n",
      "Processing Download Third Dataset ....\n",
      "Found download URL: https://www.abs.gov.au/methodologies/data-region-methodology/2011-24/14100DO0003_2011-24.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14100DO0003_2011-24.xlsx: 100%|██████████| 19.7M/19.7M [00:01<00:00, 14.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download completed: 14100DO0003_2011-24.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "retrieve_data_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469afbad",
   "metadata": {},
   "source": [
    "<h1>Data Integration and Cleaning Data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "602a59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cleaningData:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def show_null_data(self, data:pd.DataFrame):\n",
    "        '''\n",
    "            Showing Null Data for each column in a single dataframe\n",
    "\n",
    "            Args:\n",
    "                data: Dataframe to analyze\n",
    "\n",
    "        '''\n",
    "        null_count = data.isna().sum()\n",
    "        null_percent = data.isna().sum() / len(data)\n",
    "        null_percent = null_percent.apply(lambda x: f\"{x:.1%}\")\n",
    "        print(\"Total Duplicated Count: \", data.duplicated().sum())\n",
    "        results = pd.concat([null_count, null_percent], axis=1)\n",
    "        results.columns = ['Null Total Count', 'Null Percentage']\n",
    "        return results\n",
    "    \n",
    "    def split_categorical_numerical(self, data:pd.DataFrame):\n",
    "        '''\n",
    "            Spliting Categorical and Numerical Columns\n",
    "\n",
    "            Args:\n",
    "                data: Dataframe\n",
    "        '''\n",
    "        numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "        categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "        \n",
    "        return numerical_cols, categorical_cols\n",
    "    \n",
    "    def replacing_value(self,df:pd.DataFrame, existing_value, new_value):\n",
    "        df.replace(existing_value, new_value, inplace=True)\n",
    "        return None\n",
    "\n",
    "    def filling_null_value(self, df:pd.DataFrame, replacing_value):\n",
    "        cat_imputer = SimpleImputer(strategy='constant', fill_value=replacing_value)\n",
    "        df = cat_imputer.fit_transform(df)\n",
    "        \n",
    "    @staticmethod\n",
    "    def clean_power_name(row: str) -> str:\n",
    "        if not isinstance(row, str):\n",
    "            return row\n",
    "\n",
    "        # 1. Remove company suffixes and parentheses\n",
    "        row_clean = re.sub(r\"\\b(Pty|Ltd|Limited|Inc)\\b\", \"\", row, flags=re.IGNORECASE)\n",
    "        row_clean = re.sub(r\"\\(.*?\\)\", \"\", row_clean)\n",
    "\n",
    "        # 2. Normalize all dashes to single spaces\n",
    "        row_clean = re.sub(r\"[-–]+\", \" \", row_clean)\n",
    "\n",
    "        # 3. Remove SGU mentions\n",
    "        row_clean = re.sub(r\"\\b[wW]\\s*SGU\\b\", \"\", row_clean)\n",
    "\n",
    "        # 4. Remove capacity mentions\n",
    "        row_clean = re.sub(r\"\\d+\\.?\\d*\\s*(kW|MW)\", \"\", row_clean, flags=re.IGNORECASE)\n",
    "\n",
    "        # 5. Remove technology keywords\n",
    "        row_clean = re.sub(r\"\\b(Solar|Wind|Cogeneration)\\b\", \"\", row_clean, flags=re.IGNORECASE)\n",
    "\n",
    "        # 6. Remove state codes\n",
    "        row_clean = re.sub(r\"\\b(QLD|NSW|VIC|SA|WA|TAS|ACT|NT)\\b\", \"\", row_clean)\n",
    "\n",
    "        # 7. Clean up multiple spaces\n",
    "        row_clean = re.sub(r\"\\s+\", \" \", row_clean).strip()\n",
    "\n",
    "        # 8. Title case the result\n",
    "        return row_clean.title()\n",
    "    \n",
    "    def map_state(self, code):\n",
    "        \"\"\"Return state/territory for ABS codes (LGA or SA2).\"\"\"\n",
    "        \n",
    "        state_map = {\n",
    "        \"1\": \"NSW\",\n",
    "        \"2\": \"VIC\",\n",
    "        \"3\": \"QLD\",\n",
    "        \"4\": \"SA\",\n",
    "        \"5\": \"WA\",\n",
    "        \"6\": \"TAS\",\n",
    "        \"7\": \"NT\",\n",
    "        \"8\": \"ACT\",\n",
    "        \"9\": \"OT\",\n",
    "        \"AUS\": \"AUS\"\n",
    "    }\n",
    "    \n",
    "        code = str(code).strip()\n",
    "        \n",
    "        # Direct match (AUS, 1–9)\n",
    "        if code in state_map:\n",
    "            return state_map[code]\n",
    "        \n",
    "        # Alphanumeric SA4 codes (e.g., 1GSYD → first digit = state)\n",
    "        if code[0].isdigit():\n",
    "            return state_map.get(code[0], \"UNK\")\n",
    "        \n",
    "        # Pure numeric codes (SA2, SA3, LGA)\n",
    "        if code.isdigit():\n",
    "            num = int(code)\n",
    "            if num < 20000:   # NSW\n",
    "                return \"NSW\"\n",
    "            elif num < 30000: # VIC\n",
    "                return \"VIC\"\n",
    "            elif num < 40000: # QLD\n",
    "                return \"QLD\"\n",
    "            elif num < 50000: # SA\n",
    "                return \"SA\"\n",
    "            elif num < 60000: # WA\n",
    "                return \"WA\"\n",
    "            elif num < 70000: # TAS\n",
    "                return \"TAS\"\n",
    "            elif num < 80000: # NT\n",
    "                return \"NT\"\n",
    "            elif num < 90000: # ACT\n",
    "                return \"ACT\"\n",
    "            else:\n",
    "                return \"OT\"\n",
    "        \n",
    "        return \"UNK\"\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6151a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaning_data = cleaningData()\n",
    "\n",
    "# ---------------------------------- Read Dataset ----------------------#\n",
    "# first url file\n",
    "df_nger = pd.read_csv(\"ID0243.csv\")\n",
    "    \n",
    "# second url file\n",
    "df_cer_1_approved = pd.read_csv(\"power-stations-and-projects-accredited.csv\")\n",
    "df_cer_1_commited = pd.read_csv(\"power-stations-and-projects-committed.csv\")\n",
    "df_cer_1_probable = pd.read_csv(\"power-stations-and-projects-probable.csv\")\n",
    "\n",
    "# third url file\n",
    "df_abs = pd.ExcelFile(\"14100DO0003_2011-24.xlsx\")\n",
    "statistical_area = pd.read_excel(df_abs, 'Table 1', header=[6])\n",
    "lga_area = pd.read_excel(df_abs, 'Table 2', header=[6])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d12e21",
   "metadata": {},
   "source": [
    "#### Cleaning Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a5144f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92c7665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------- Clean NGER.ID0243.csv ------------------------#\n",
    "\n",
    "\n",
    "# keep the type of F only\n",
    "df_nger = df_nger[df_nger['Type'] == 'F'].copy()\n",
    "\n",
    "cleaning_data.replacing_value(df_nger, '-', np.nan)\n",
    "\n",
    "columns_to_drop = [\n",
    "    'Electricity production GJ', \n",
    "    'Total emissions t CO2 e', \n",
    "    'Reporting entity', \n",
    "    'Important notes', \n",
    "    'Grid connected',\n",
    "    'Type'\n",
    "]\n",
    "\n",
    "df_nger_numerical, df_nger_categorical = cleaning_data.split_categorical_numerical(df_nger) \n",
    "\n",
    "cleaning_data.filling_null_value(df_nger[df_nger_categorical], \"Unknown\")\n",
    "\n",
    "df_nger_cleaning = df_nger.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c87b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Duplicated Count:  0\n",
      "Total Duplicated Count:  0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Null Total Count</th>\n",
       "      <th>Null Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Project Name</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MW Capacity</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fuel Source</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Null Total Count Null Percentage\n",
       "Project Name                 0            0.0%\n",
       "State                        0            0.0%\n",
       "MW Capacity                  0            0.0%\n",
       "Fuel Source                  0            0.0%"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------ Cleaning CER Approved Dataset --------------#\n",
    "\n",
    "cleaning_data.replacing_value(df_cer_1_approved, '-', np.nan)\n",
    "\n",
    "df_cer_1_approved[\"power_station_name_clean\"] = df_cer_1_approved[\"Power station name\"].apply(cleaning_data.clean_power_name)\n",
    "\n",
    "\n",
    "# ------------------------------ Cleaning CER Committed Dataset --------------#\n",
    "\n",
    "cleaning_data.replacing_value(df_cer_1_commited, '-', np.nan)\n",
    "\n",
    "if \"Committed Date (Month/Year)\" in df_cer_1_commited.columns:\n",
    "    df_cer_1_commited[\"committed_date\"] = pd.to_datetime(\n",
    "        df_cer_1_commited[\"Committed Date (Month/Year)\"], format=\"%b-%Y\", errors=\"coerce\"\n",
    "    ).dt.strftime(\"%Y-%m-01\")  # first day of the month\n",
    "\n",
    "cleaning_data.show_null_data(df_cer_1_commited)\n",
    "\n",
    "\n",
    "# ------------------------------ Cleaning CER Probable Dataset --------------#\n",
    "\n",
    "cleaning_data.replacing_value(df_cer_1_probable, '-', np.nan)\n",
    "cleaning_data.show_null_data(df_cer_1_probable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a39f19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2h/b49chrcn56z4tf_5y0zw27740000gn/T/ipykernel_94505/3871997723.py:34: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(existing_value, new_value, inplace=True)\n",
      "/var/folders/2h/b49chrcn56z4tf_5y0zw27740000gn/T/ipykernel_94505/1252239592.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  statistical_area['State'] = statistical_area['Code'].apply(cleaning_data.map_state)\n",
      "/var/folders/2h/b49chrcn56z4tf_5y0zw27740000gn/T/ipykernel_94505/3871997723.py:34: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(existing_value, new_value, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------- Cleaning ABS Dataset -----------------------#\n",
    "\n",
    "# only get the important rows\n",
    "statistical_area = statistical_area.iloc[:-7]\n",
    "lga_area = lga_area.iloc[:-7]\n",
    "\n",
    "cleaning_data.replacing_value(statistical_area, '-', np.nan)\n",
    "\n",
    "statistical_area[\"Year\"] = pd.to_numeric(statistical_area[\"Year\"], errors=\"coerce\").astype(\"Int64\") \n",
    "statistical_area['State'] = statistical_area['Code'].apply(cleaning_data.map_state)\n",
    "\n",
    "lga_area = lga_area.drop(columns=[\n",
    "    \"Total number of businesses\", \n",
    "    \"Number of non-employing business entries\",\n",
    "    \"Number of employing business entries: 1-4 employees\",\n",
    "    \"Number of employing business entries: 5-19 employees\",\n",
    "    \"Number of employing business entries: 20 or more employees\",\n",
    "    \"Total number of business entries\",\n",
    "    \"Number of non-employing business exits\",\n",
    "    \"Number of employing business exits: 1-4 employees\",\n",
    "    \"Number of employing business exits: 5-19 employees\",\n",
    "    \"Number of employing business exits: 20 or more employees\",\n",
    "    \"Total number of business exits\",\n",
    "    \"Information media and telecommunications (no.)\",\n",
    "    \"Financial and insurance services (no.)\",\n",
    "    \"Rental, hiring and real estate services (no.)\",\n",
    "    \"Professional, scientific and technical services (no.)\",\n",
    "    \"Administrative and support services (no.)\",\n",
    "    \"Public administration and safety (no.)\",\n",
    "    \"Education and training (no.)\",\n",
    "    \"Arts and recreation services (no.)\",\n",
    "    \"Other services (no.)\",\n",
    "    \"Currently unknown (no.)\",\n",
    "    \"Number of business entries with turnover of zero to less than $50k\",\n",
    "    \"Number of business entries with turnover of $50k to less than $200k\",\n",
    "    \"Number of business entries with turnover of $200k to less than $2m\",\n",
    "    \"Number of business entries with turnover of $2m to less than $5m\",\n",
    "    \"Number of business entries with turnover of $5m to less than $10m\",\n",
    "    \"Number of business entries with turnover of $10m or more\",\n",
    "    \"Number of business exits with turnover of zero to less than $50k\",\n",
    "    \"Number of business exits with turnover of $50k to less than $200k\",\n",
    "    \"Number of business exits with turnover of $200k to less than $2m\",\n",
    "    \"Number of business exits with turnover of $2m to less than $5m\",\n",
    "    \"Number of business exits with turnover of $5m to less than $10m\",\n",
    "    \"Number of business exits with turnover of $10m or more\",\n",
    "    \"Value of private sector houses ($m)\",\n",
    "    \"Value of private sector dwellings excluding houses ($m)\",\n",
    "    \"Total value of private sector dwelling units ($m)\",\n",
    "    \"Value of residential building ($m)\",\n",
    "    \"Value of non-residential building ($m)\",\n",
    "    \"Value of total building ($m)\",\n",
    "    \"Number of established house transfers (no.)\",\n",
    "    \"Median price of established house transfers ($)\",\n",
    "    \"Number of attached dwelling transfers (no.)\",\n",
    "    \"Median price of attached dwelling transfers ($)\",\n",
    "    \"Debtors entering business related personal insolvencies (no.)\",\n",
    "    \"Debtors entering non-business related personal insolvencies (no.)\",\n",
    "    \"Total debtors entering personal insolvencies (no.)\",\n",
    "    \"Managers (no.)\",\n",
    "    \"Professionals (no.)\",\n",
    "    \"Technicians and trades workers (no.)\",\n",
    "    \"Community and personal service workers (no.)\",\n",
    "    \"Clerical and administrative workers (no.)\",\n",
    "    \"Sales workers (no.)\",\n",
    "    \"Machinery operators and drivers (no.)\",\n",
    "    \"Labourers (no.)\",\n",
    "    \"Debtors with other or unknown occupations (no.)\",\n",
    "    \"Area of holding - total area (ha)\",\n",
    "    \"Dairy cattle - total (no.)\",\n",
    "    \"Meat cattle - total (no.)\",\n",
    "    \"Sheep and lambs - total (no.)\",\n",
    "    \"Pigs - total (no.)\",\n",
    "    \"Meat chickens - total (no.)\",\n",
    "    \"Broadacre crops - total area (ha)\",\n",
    "    \"Vegetables - total area (ha)\",\n",
    "    \"Orchard fruit trees and nut trees (produce intended for sale) - total area - (ha)\",\n",
    "    \"Agricultural production - total gross value ($m)\",\n",
    "    \"Crops - total gross value ($m)\",\n",
    "    \"Livestock slaughtered and other disposals - total gross value ($m)\"\n",
    "])\n",
    "\n",
    "statistical_area = statistical_area.drop(columns=[\n",
    "    \"Total number of businesses\", \n",
    "    \"Number of non-employing business entries\",\n",
    "    \"Number of employing business entries: 1-4 employees\",\n",
    "    \"Number of employing business entries: 5-19 employees\",\n",
    "    \"Number of employing business entries: 20 or more employees\",\n",
    "    \"Total number of business entries\",\n",
    "    \"Number of non-employing business exits\",\n",
    "    \"Number of employing business exits: 1-4 employees\",\n",
    "    \"Number of employing business exits: 5-19 employees\",\n",
    "    \"Number of employing business exits: 20 or more employees\",\n",
    "    \"Total number of business exits\",\n",
    "    \"Information media and telecommunications (no.)\",\n",
    "    \"Financial and insurance services (no.)\",\n",
    "    \"Rental, hiring and real estate services (no.)\",\n",
    "    \"Professional, scientific and technical services (no.)\",\n",
    "    \"Administrative and support services (no.)\",\n",
    "    \"Public administration and safety (no.)\",\n",
    "    \"Education and training (no.)\",\n",
    "    \"Arts and recreation services (no.)\",\n",
    "    \"Other services (no.)\",\n",
    "    \"Currently unknown (no.)\",\n",
    "    \"Number of business entries with turnover of zero to less than $50k\",\n",
    "    \"Number of business entries with turnover of $50k to less than $200k\",\n",
    "    \"Number of business entries with turnover of $200k to less than $2m\",\n",
    "    \"Number of business entries with turnover of $2m to less than $5m\",\n",
    "    \"Number of business entries with turnover of $5m to less than $10m\",\n",
    "    \"Number of business entries with turnover of $10m or more\",\n",
    "    \"Number of business exits with turnover of zero to less than $50k\",\n",
    "    \"Number of business exits with turnover of $50k to less than $200k\",\n",
    "    \"Number of business exits with turnover of $200k to less than $2m\",\n",
    "    \"Number of business exits with turnover of $2m to less than $5m\",\n",
    "    \"Number of business exits with turnover of $5m to less than $10m\",\n",
    "    \"Number of business exits with turnover of $10m or more\",\n",
    "    \"Value of private sector houses ($m)\",\n",
    "    \"Value of private sector dwellings excluding houses ($m)\",\n",
    "    \"Total value of private sector dwelling units ($m)\",\n",
    "    \"Value of residential building ($m)\",\n",
    "    \"Value of non-residential building ($m)\",\n",
    "    \"Value of total building ($m)\",\n",
    "    \"Number of established house transfers (no.)\",\n",
    "    \"Median price of established house transfers ($)\",\n",
    "    \"Number of attached dwelling transfers (no.)\",\n",
    "    \"Median price of attached dwelling transfers ($)\",\n",
    "    \"Debtors entering business related personal insolvencies (no.)\",\n",
    "    \"Debtors entering non-business related personal insolvencies (no.)\",\n",
    "    \"Total debtors entering personal insolvencies (no.)\",\n",
    "    \"Managers (no.)\",\n",
    "    \"Professionals (no.)\",\n",
    "    \"Technicians and trades workers (no.)\",\n",
    "    \"Community and personal service workers (no.)\",\n",
    "    \"Clerical and administrative workers (no.)\",\n",
    "    \"Sales workers (no.)\",\n",
    "    \"Machinery operators and drivers (no.)\",\n",
    "    \"Labourers (no.)\",\n",
    "    \"Debtors with other or unknown occupations (no.)\",\n",
    "    \"Area of holding - total area (ha)\",\n",
    "    \"Dairy cattle - total (no.)\",\n",
    "    \"Meat cattle - total (no.)\",\n",
    "    \"Sheep and lambs - total (no.)\",\n",
    "    \"Pigs - total (no.)\",\n",
    "    \"Meat chickens - total (no.)\",\n",
    "    \"Broadacre crops - total area (ha)\",\n",
    "    \"Vegetables - total area (ha)\",\n",
    "    \"Orchard fruit trees and nut trees (produce intended for sale) - total area - (ha)\",\n",
    "    \"Agricultural production - total gross value ($m)\",\n",
    "    \"Crops - total gross value ($m)\",\n",
    "    \"Livestock slaughtered and other disposals - total gross value ($m)\",\n",
    "    \"Houses - additions (no.)\",\n",
    "    \"Houses - removals (no.)\",\n",
    "    \"Townhouses - additions (no.)\",\n",
    "    \"Townhouses - removals (no.)\",\n",
    "    \"Apartments - additions (no.)\",\n",
    "    \"Apartments - removals (no.)\",\n",
    "    \"Total dwelling additions (no.)\",\n",
    "    \"Total dwelling removals (no.)\"\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "cleaning_data.replacing_value(lga_area, '-', np.nan)\n",
    "\n",
    "lga_area[\"Year\"] = pd.to_numeric(lga_area[\"Year\"], errors=\"coerce\").astype(\"Int64\") \n",
    "\n",
    "lga_area['State'] = lga_area['Code'].apply(cleaning_data.map_state)\n",
    "\n",
    "statistical_area[\"Year\"] = pd.to_numeric(statistical_area[\"Year\"], errors=\"coerce\").astype(\"Int64\") \n",
    "\n",
    "statistical_area['State'] = statistical_area['Code'].apply(cleaning_data.map_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a53166e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exclude_col = ['Code', 'Label', 'State']\n",
    "# Convert all other columns to numeric\n",
    "numeric_cols_lga = [c for c in lga_area.columns if c not in exclude_col]\n",
    "lga_area[numeric_cols_lga] = lga_area[numeric_cols_lga].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "numeric_cols_stat = [c for c in statistical_area.columns if c not in exclude_col]\n",
    "statistical_area[numeric_cols_stat] = statistical_area[numeric_cols_stat].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da685bb",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6269a40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoAPI:\n",
    "    def __init__(self, api_key):\n",
    "        self.gmaps = googlemaps.Client(key=api_key)\n",
    "\n",
    "    def get_coordinates(self, query: str):\n",
    "        \"\"\"Return (lat, lon, types) or (None, None, None) if not found\"\"\"\n",
    "        try:\n",
    "            results = self.gmaps.geocode(query)\n",
    "            if results:\n",
    "                location = results[0]['geometry']['location']\n",
    "                lat = location['lat']\n",
    "                lon = location['lng']\n",
    "                place_types = results[0].get('types', [])\n",
    "                return lat, lon, place_types\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error geocoding '{query}': {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    def geocode_dataframe(self, df: pd.DataFrame, col_names, delay: float = 0.1):\n",
    "        \"\"\"\n",
    "        Add latitude/longitude tuple and Place_Types columns to a DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: pandas DataFrame\n",
    "            col_names: str or list of column names to use for geocoding query\n",
    "            delay: float, delay between API calls to avoid rate limits\n",
    "        Returns:\n",
    "            DataFrame with 'geo_location' and 'Place_Types' columns added\n",
    "        \"\"\"\n",
    "        if isinstance(col_names, str):\n",
    "            col_names = [col_names]\n",
    "\n",
    "        points, types_list = [], []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            query = \", \".join([str(row[col]) for col in col_names if pd.notna(row[col])])\n",
    "            print(query)\n",
    "            if not query:\n",
    "                points.append(None)\n",
    "                types_list.append(None)\n",
    "                continue\n",
    "\n",
    "            lat, lon, types_ = self.get_coordinates(query)\n",
    "            \n",
    "            # Just store (lat, lon)\n",
    "            if lat is not None and lon is not None:\n",
    "                points.append((lat, lon))  \n",
    "            else:\n",
    "                points.append(None)\n",
    "\n",
    "            types_list.append(types_)\n",
    "            time.sleep(delay)\n",
    "\n",
    "        df[\"geo_location\"] = points\n",
    "        df[\"Place_Types\"] = types_list\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cef2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laura Johnson Home, Townview, 4825, QLD\n",
      "Leppington, 2179, NSW\n",
      "Quakers Hillside Care Community, 2763, NSW\n",
      "Rest Nominees, 3008, VIC\n",
      "Retail First Mt Ommaney, 4074, QLD\n",
      "Woolworths Hcfdc Heathwood, 4110, QLD\n",
      "Woolworths Kings Meadow 7210, 7249, TAS\n",
      "Clayton Church Homes Elizabeth Park, 5113, SA\n",
      "Dalwood Children'S Home, 2092, NSW\n",
      "Haighs Proprietary Salisbury South, 5106, SA\n",
      "Lindt & Sprungli, 2765, NSW\n",
      "Midfield Trading, 5010, SA\n",
      "Wilandra Farms Clydebank, 3851, VIC\n",
      "Balickera Wps, 2324, NSW\n",
      "The Creeks Pipeline Company, 5259, SA\n",
      "Lai Industries, 5010, SA\n",
      "Clayton Church Homes Prospect, 5082, SA\n",
      "Kerang Plant, 3579, VIC\n",
      "Red Valley Farms, 4871, QLD\n",
      "Region Group Warnbro Central, 6169, WA\n",
      "Southern Steel Dandenong South, 3175, VIC\n",
      "Southern Steel Pooraka, 5095, SA\n",
      "Clayton Church Homes Magill, 5072, SA\n",
      "Au186 Oakdale West 5B, 2178, NSW\n",
      "Carosue, 6440, WA\n",
      "Linfox Regency Park, 5010, SA\n",
      "Pwr Hybrid Cataby, 6507, WA\n",
      "Blacktown Warehouse 1, 2148, NSW\n",
      "Blacktown Warehouse 2, 2148, NSW\n",
      "Hcs, 4014, QLD\n",
      "Intertek Townsville, 4818, QLD\n",
      "Islhd Bulli Hospital, 2516, NSW\n",
      "Region Group Treendale, 6233, WA\n",
      "Woolworths Dc Moorebank Rdc 7950, 2170, NSW\n",
      "Bunnings Padstow, 2211, NSW\n",
      "Five Star Seafoods, 5291, SA\n",
      "Morgan Sawmill, 5491, SA\n",
      "Northpoint Shopping Centre, 4350, QLD\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------Starting Google Map API ---------------- #\n",
    "# load from environment variable\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Google API\n",
    "API_KEY = os.getenv(\"GOOGLE_MAPS_API_KEY\")\n",
    "geo_api = GeoAPI(API_KEY)\n",
    "\n",
    "# Geocode DataFrame\n",
    "df_geo_approved = geo_api.geocode_dataframe(df_cer_1_approved, [\"power_station_name_clean\", \"Postcode\", \"State\"])\n",
    "df_geo_commited = geo_api.geocode_dataframe(df_cer_1_commited, [\"Project Name\", \"State \"])\n",
    "df_geo_probable= geo_api.geocode_dataframe(df_cer_1_probable, [\"Project Name\", \"State \"])\n",
    "\n",
    "for col in df_geo_approved.columns:\n",
    "    df_geo_approved[col] = df_geo_approved[col].apply(lambda x: ','.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "for col in df_geo_commited.columns:\n",
    "    df_geo_commited[col] = df_geo_commited[col].apply(lambda x: ','.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "for col in df_geo_probable.columns:\n",
    "    df_geo_probable[col] = df_geo_probable[col].apply(lambda x: ','.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d63300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_valid(place_types):\n",
    "    if not isinstance(place_types, str) or not place_types.strip():\n",
    "        return 'Not Valid'\n",
    "    types = [t.strip() for t in place_types.split(',')]\n",
    "    return 'Valid' if 'establishment' in types or 'point_of_interest' in types else 'Not Valid'\n",
    "\n",
    "# Check if the Places Type is valid or not\n",
    "df_geo_approved['valid_flag'] = df_geo_approved['Place_Types'].apply(is_valid)\n",
    "df_geo_commited['valid_flag'] = df_geo_commited['Place_Types'].apply(is_valid)\n",
    "df_geo_probable['valid_flag'] = df_geo_probable['Place_Types'].apply(is_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5b3c6",
   "metadata": {},
   "source": [
    "#### Process Creating Schema in Duck DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5b891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConnectDB:\n",
    "    \"\"\"DuckDB database connection and operations.\"\"\"\n",
    "\n",
    "    SQL_TYPES = {\n",
    "        'object': 'VARCHAR',\n",
    "        'int64': 'INTEGER',\n",
    "        'float64': 'DOUBLE',\n",
    "        'bool': 'BOOLEAN',\n",
    "        'datetime64[ns]': 'TIMESTAMP',  \n",
    "        'date': 'DATE',\n",
    "        'geometry': 'GEOMETRY',  \n",
    "        'point': 'POINT'                \n",
    "    }\n",
    "\n",
    "    def __init__(self, db_path: str = 'mydatabase.duckdb'):\n",
    "        self.db_path = db_path\n",
    "        self.connection = None\n",
    "        self.open()  # open immediately\n",
    "\n",
    "    def open(self):\n",
    "        \"\"\"Open DuckDB connection if not already open.\"\"\"\n",
    "        if self.connection is None:\n",
    "            self.connection = duckdb.connect(self.db_path)\n",
    "            # load spatial extension\n",
    "            self.connection.execute(\"INSTALL spatial;\")\n",
    "            self.connection.execute(\"LOAD spatial;\")\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close DuckDB connection if open.\"\"\"\n",
    "        if self.connection is not None:\n",
    "            self.connection.close()\n",
    "            self.connection = None\n",
    "\n",
    "    def create_table(self, df: pd.DataFrame, table_name: str,\n",
    "                 primary_key: str = None,\n",
    "                 auto_increment: bool = False,\n",
    "                 foreign_keys: dict[str, tuple[str, str]] = None,\n",
    "                 column_types: dict = None,\n",
    "                 not_null_foreign_keys: bool = True):\n",
    "        \"\"\"\n",
    "        Create a DuckDB table with specified column types, auto-increment PK, and FK constraints.\n",
    "\n",
    "        Args:\n",
    "            df: pandas DataFrame (used to get column names).\n",
    "            table_name: str, table name to create.\n",
    "            primary_key: str, column to set as primary key.\n",
    "            auto_increment: bool, whether primary key should auto-increment.\n",
    "            foreign_keys: dict {col_name: (ref_table, ref_col)}.\n",
    "            column_types: dict {col_name: sql_type}.\n",
    "            not_null_foreign_keys: bool, if True, foreign key columns will be NOT NULL.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.open()\n",
    "            columns_sql = []\n",
    "\n",
    "            # Create sequence for auto-increment if needed\n",
    "            if auto_increment and primary_key:\n",
    "                sequence_name = f\"{table_name}_{primary_key}_seq\"\n",
    "                self.connection.execute(f\"CREATE SEQUENCE IF NOT EXISTS {sequence_name} START 1;\")\n",
    "\n",
    "            for col_name in df.columns:\n",
    "                # Determine SQL type\n",
    "                sql_type = 'VARCHAR'\n",
    "                if column_types and col_name in column_types:\n",
    "                    sql_type = column_types[col_name]\n",
    "                elif col_name in df.dtypes:\n",
    "                    sql_type = self.SQL_TYPES.get(str(df[col_name].dtype), 'VARCHAR')\n",
    "\n",
    "                # Use sequence for auto-increment primary key\n",
    "                if auto_increment and col_name == primary_key:\n",
    "                    sql_type = f'INTEGER DEFAULT nextval(\\'{sequence_name}\\')'\n",
    "\n",
    "                # Apply NOT NULL for foreign keys if requested\n",
    "                if foreign_keys and col_name in foreign_keys and not_null_foreign_keys:\n",
    "                    sql_type += \" NOT NULL\"\n",
    "\n",
    "                columns_sql.append(f'\"{col_name}\" {sql_type}')\n",
    "\n",
    "            # Add primary key constraint\n",
    "            if primary_key:\n",
    "                columns_sql.append(f'PRIMARY KEY(\"{primary_key}\")')\n",
    "\n",
    "            # Add foreign key constraints\n",
    "            if foreign_keys:\n",
    "                for col, (ref_table, ref_col) in foreign_keys.items():\n",
    "                    columns_sql.append(f'FOREIGN KEY(\"{col}\") REFERENCES {ref_table}(\"{ref_col}\")')\n",
    "\n",
    "            sql = f\"CREATE TABLE IF NOT EXISTS {table_name} (\\n    {',\\n    '.join(columns_sql)}\\n);\"\n",
    "            print(\"Executing SQL:\\n\", sql)\n",
    "            self.connection.execute(sql)\n",
    "            print(f\"Table '{table_name}' created successfully.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating table '{table_name}': {e}\")\n",
    "        finally:\n",
    "            self.close()\n",
    "\n",
    "    def insert_data(self, df: pd.DataFrame, table_name: str, column_map: dict):\n",
    "        \"\"\"Insert DataFrame into DuckDB table with optional column mapping.\"\"\"\n",
    "        try:\n",
    "            self.open()  # ensure connection is open\n",
    "\n",
    "            select_clause = \", \".join([f'\"{df_col}\" AS \"{table_col}\"' for df_col, table_col in column_map.items()])\n",
    "            self.connection.register(\"df_temp\", df)\n",
    "            sql = f\"\"\"\n",
    "                INSERT INTO {table_name} ({', '.join(column_map.values())})\n",
    "                SELECT {select_clause} FROM df_temp;\n",
    "            \"\"\"\n",
    "            self.connection.execute(sql)\n",
    "            print(f\"Inserted {len(df)} rows into {table_name}\")\n",
    "        except Exception as e:\n",
    "            print(\"Error inserting data:\", e)\n",
    "        finally:\n",
    "             self.close()\n",
    "             \n",
    "    def generate_numeric_uuid(self, existing_ids=None):\n",
    "        \"\"\"\n",
    "        Generate a positive numeric UUID (24-bit) and ensure no duplicates\n",
    "        in the provided set of existing_ids.\n",
    "        \"\"\"\n",
    "        if existing_ids is None:\n",
    "            existing_ids = set()\n",
    "\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            new_id = uuid.uuid4().int & 0xFFFFFF\n",
    "            if new_id not in existing_ids:\n",
    "                existing_ids.add(new_id)\n",
    "                return new_id\n",
    "            attempt += 1\n",
    "            if attempt > 10:\n",
    "                raise ValueError(\"Unable to generate a unique UUID after 10 attempts\")\n",
    "\n",
    "\n",
    "    def insert_dataframe_multi_table_rowwise(self, df: pd.DataFrame, tables: list, delay: float = 0.0):\n",
    "        \"\"\"\n",
    "        Insert a DataFrame into multiple tables row by row with foreign keys.\n",
    "        Uses numeric UUIDs instead of AUTOINCREMENT.\n",
    "        Handles MultiIndex columns by flattening.\n",
    "        Ensures parent-child FK resolution even with duplicate rows.\n",
    "\n",
    "        Args:\n",
    "            df: pandas DataFrame (source dataset)\n",
    "            tables: List of table metadata dicts:\n",
    "                - name: table name\n",
    "                - primary_key: table PK\n",
    "                - auto_increment: bool (ignored)\n",
    "                - dataset_identifier: column name in dataset for linking parent-child\n",
    "                - foreign_keys: {db_col: (parent_table, parent_pk, dataset_identifier_col)}\n",
    "                - column_map: {df_col: db_col}\n",
    "            delay: optional delay between rows\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.open()\n",
    "            # Ensure spatial extension is loaded\n",
    "            self.connection.execute(\"INSTALL spatial;\")\n",
    "            self.connection.execute(\"LOAD spatial;\")\n",
    "\n",
    "            # Flatten MultiIndex columns if present\n",
    "            if isinstance(df.columns, pd.MultiIndex):\n",
    "                df.columns = ['_'.join(map(str, col)).strip() if isinstance(col, tuple) else str(col) for col in df.columns]\n",
    "\n",
    "            # Store PKs for FK resolution\n",
    "            pk_store = {}\n",
    "\n",
    "            # Keep track of generated UUIDs to prevent duplicates\n",
    "            generated_uuids = set()\n",
    "\n",
    "            # Create a unique row identifier column to handle duplicates\n",
    "            unique_row_id_col = \"_row_identifier\"\n",
    "            df[unique_row_id_col] = df.index.astype(str)\n",
    "\n",
    "            for idx, row in df.iterrows():\n",
    "                for tbl in tables:\n",
    "                    table_name = tbl['name']\n",
    "                    primary_key = tbl.get('primary_key')\n",
    "                    foreign_keys = tbl.get('foreign_keys', {})\n",
    "                    column_map = tbl.get('column_map', {})\n",
    "                    dataset_identifier_col = tbl.get(\"dataset_identifier\", unique_row_id_col)\n",
    "\n",
    "                    if not column_map:\n",
    "                        column_map = {c: c for c in df.columns}\n",
    "\n",
    "                    db_cols = []\n",
    "                    values = []\n",
    "\n",
    "                    # ----- UUID as primary key -----\n",
    "                    new_id = None\n",
    "                    if primary_key:\n",
    "                        new_id = self.generate_numeric_uuid(existing_ids=generated_uuids)\n",
    "                        db_cols.append(primary_key)\n",
    "                        values.append(str(new_id))\n",
    "\n",
    "                    for df_col, db_col in column_map.items():\n",
    "                        # If MultiIndex flattened, ensure key is string\n",
    "                        if isinstance(df_col, tuple):\n",
    "                            df_col = '_'.join(map(str, df_col))\n",
    "\n",
    "                        val = row.get(df_col, None)\n",
    "\n",
    "                        # ----- FOREIGN KEY HANDLING -----\n",
    "                        if db_col in foreign_keys:\n",
    "                            parent_table, parent_pk, parent_identifier_col = foreign_keys[db_col]\n",
    "                            if parent_table not in pk_store:\n",
    "                                raise ValueError(f\"Parent table '{parent_table}' not inserted yet.\")\n",
    "\n",
    "                            parent_rows = pk_store[parent_table][\n",
    "                                pk_store[parent_table][parent_identifier_col] == row[parent_identifier_col]\n",
    "                            ]\n",
    "                            if parent_rows.empty:\n",
    "                                raise ValueError(f\"No parent found for row {idx} in table '{parent_table}'\")\n",
    "                            val = int(parent_rows[parent_pk].values[0])\n",
    "                            db_cols.append(db_col)\n",
    "                            values.append(str(val))\n",
    "                            continue\n",
    "\n",
    "                        # ----- DATE HANDLING -----\n",
    "                        if 'date' in df_col.lower():\n",
    "                            if pd.notna(val):\n",
    "                                try:\n",
    "                                    val_dt = pd.to_datetime(val, dayfirst=True)\n",
    "                                    val = f\"'{val_dt.strftime('%Y-%m-%d')}'\"\n",
    "                                except Exception:\n",
    "                                    val = 'NULL'\n",
    "                            else:\n",
    "                                val = 'NULL'\n",
    "                            db_cols.append(db_col)\n",
    "                            values.append(str(val))\n",
    "                            continue\n",
    "\n",
    "                        # ----- SPATIAL HANDLING -----\n",
    "                        if db_col.lower() == 'geo_location':\n",
    "                            if pd.notna(val):\n",
    "                                val_str = str(val).strip()\n",
    "                                if val_str.upper().startswith(\"ST_POINT\"):\n",
    "                                    pass\n",
    "                                elif val_str.upper().startswith(\"POINT\"):\n",
    "                                    coords = val_str.replace(\"POINT(\", \"\").replace(\")\", \"\").split()\n",
    "                                    if len(coords) == 2:\n",
    "                                        val = f\"ST_POINT({coords[0]}, {coords[1]})\"\n",
    "                                    else:\n",
    "                                        val = \"ST_POINT(0,0)\"\n",
    "                                elif val_str.startswith(\"(\") and val_str.endswith(\")\"):\n",
    "                                    coords = val_str.strip(\"()\").split(\",\")\n",
    "                                    if len(coords) == 2:\n",
    "                                        lat, lon = coords[0].strip(), coords[1].strip()\n",
    "                                        val = f\"ST_POINT({lon}, {lat})\"\n",
    "                                    else:\n",
    "                                        val = \"ST_POINT(0,0)\"\n",
    "                                else:\n",
    "                                    val = \"ST_POINT(0,0)\"\n",
    "                            else:\n",
    "                                val = \"ST_POINT(0,0)\"  # default point instead of NULL\n",
    "                            db_cols.append(db_col)\n",
    "                            values.append(str(val))\n",
    "                            continue\n",
    "\n",
    "                        # ----- OTHER TYPES -----\n",
    "                        if pd.isna(val):\n",
    "                            val = 'NULL'\n",
    "                        elif isinstance(val, str):\n",
    "                            val = f\"'{val.replace('\\'', '\\'\\'')}'\"\n",
    "                        else:\n",
    "                            val = str(val)\n",
    "                        db_cols.append(db_col)\n",
    "                        values.append(val)\n",
    "\n",
    "                    # ----- INSERT SQL -----\n",
    "                    sql = f\"\"\"\n",
    "                        INSERT INTO {table_name} ({', '.join(db_cols)})\n",
    "                        VALUES ({', '.join(values)})\n",
    "                    \"\"\"\n",
    "                    self.connection.execute(sql)\n",
    "\n",
    "                    # ----- Store PK for FK resolution -----\n",
    "                    if primary_key and new_id:\n",
    "                        df_pk = pd.DataFrame([{primary_key: new_id}])\n",
    "                        if dataset_identifier_col:\n",
    "                            df_pk[dataset_identifier_col] = [row[dataset_identifier_col]]\n",
    "                        pk_store[table_name] = (\n",
    "                            pd.concat([pk_store.get(table_name, pd.DataFrame()), df_pk])\n",
    "                            if table_name in pk_store else df_pk\n",
    "                        )\n",
    "\n",
    "                if delay > 0:\n",
    "                    time.sleep(delay)\n",
    "\n",
    "            print(\"✅ All rows inserted successfully (UUIDs used for PKs).\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"❌ Error inserting multi-table data row-wise:\", e)\n",
    "\n",
    "        finally:\n",
    "            self.close()\n",
    "    def insert_dataframe_bulk_abs_autoinc(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Bulk insert ABS dataset using auto-increment IDs.\n",
    "        Ensures no duplicate 'question' or 'state_industry_data' entries.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.open()\n",
    "\n",
    "            required_cols = ['Code', 'Label', 'State', 'Year']\n",
    "            for col in required_cols:\n",
    "                if col not in df.columns:\n",
    "                    raise KeyError(f\"❌ Missing required column '{col}'\")\n",
    "\n",
    "            question_cols = [c for c in df.columns if c not in required_cols]\n",
    "\n",
    "            # --- Step 1: Insert unique questions ---\n",
    "            existing_questions = {\n",
    "                row[0]: row[1] for row in self.connection.execute(\n",
    "                    \"SELECT question, question_id FROM question\"\n",
    "                ).fetchall()\n",
    "            }\n",
    "\n",
    "            question_map = existing_questions.copy()\n",
    "            for q in question_cols:\n",
    "                if q not in existing_questions:\n",
    "                    self.connection.execute(\n",
    "                        \"INSERT INTO question (question) VALUES (?)\", [q]\n",
    "                    )\n",
    "                    q_id = self.connection.execute(\n",
    "                        \"SELECT question_id FROM question WHERE question = ?\", [q]\n",
    "                    ).fetchone()[0]\n",
    "                    question_map[q] = q_id\n",
    "\n",
    "            # --- Step 2: Insert unique state_industry_data ---\n",
    "            existing_state_data = {\n",
    "                (row[0], row[1]): row[2] for row in self.connection.execute(\n",
    "                    \"SELECT label_code, state_code, state_data_id FROM state_industry_data\"\n",
    "                ).fetchall()\n",
    "            }\n",
    "\n",
    "            state_data_map = existing_state_data.copy()\n",
    "            for _, row in df[required_cols].drop_duplicates().iterrows():\n",
    "                key = (row['Code'], row['State'])\n",
    "                if key not in existing_state_data:\n",
    "                    self.connection.execute(\n",
    "                        \"INSERT INTO state_industry_data (label_code, label, state_code) VALUES (?, ?, ?)\",\n",
    "                        [row['Code'], row['Label'], row['State']]\n",
    "                    )\n",
    "                    s_id = self.connection.execute(\n",
    "                        \"SELECT state_data_id FROM state_industry_data WHERE label_code=? AND state_code=?\",\n",
    "                        [row['Code'], row['State']]\n",
    "                    ).fetchone()[0]\n",
    "                    state_data_map[key] = s_id\n",
    "\n",
    "            # --- Step 3: Insert list_answer_data ---\n",
    "            for _, row in df.iterrows():\n",
    "                s_id = state_data_map[(row['Code'], row['State'])]\n",
    "                for q in question_cols:\n",
    "                    q_id = question_map[q]\n",
    "                    answer = row[q] if pd.notna(row[q]) else None\n",
    "                    # Convert numeric values if possible\n",
    "                    if answer is not None:\n",
    "                        try:\n",
    "                            answer = float(answer)\n",
    "                        except:\n",
    "                            answer = str(answer)\n",
    "                    self.connection.execute(\n",
    "                        \"INSERT INTO list_answer_data (answer, question_id, state_data_id, year) VALUES (?, ?, ?, ?)\",\n",
    "                        [answer, q_id, s_id, row['Year']]\n",
    "                    )\n",
    "\n",
    "            print(\"✅ ABS dataset inserted successfully using auto-increment IDs.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"❌ Error inserting ABS dataset:\", e)\n",
    "\n",
    "        finally:\n",
    "            self.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module related for database\n",
    "database = ConnectDB(\"Asgmt1.duckdb\")\n",
    "\n",
    "# -------------------------------------------- Create list of field ------------------------------ #\n",
    "column_state = ['state_code', 'name']\n",
    "\n",
    "state_abbrev = {\n",
    "    \"NSW\": \"New South Wales\",\n",
    "    \"VIC\": \"Victoria\",\n",
    "    \"QLD\": \"Queensland\",\n",
    "    \"SA\": \"South Australia\",\n",
    "    \"WA\": \"Western Australia\",\n",
    "    \"TAS\": \"Tasmania\",\n",
    "    \"NT\": \"Northern Territory\",\n",
    "    \"ACT\": \"Australian Capital Territory\",\n",
    "    \"OT\": \"Other Territories\",\n",
    "    \"AUS\": \"Australia\",\n",
    "}\n",
    "\n",
    "column_states = ['states_id', 'states']\n",
    "\n",
    "column_name_facility = ['facility_id', 'state_code', 'facility_name', 'postcode', 'source']\n",
    "\n",
    "\n",
    "column_name_facility_status = [\n",
    "    'facility_status_id',\n",
    "    'facility_id',\n",
    "    'accreditation_code',\n",
    "    'capacity_mw',\n",
    "    'accreditation_start_date',\n",
    "    'approval_date',\n",
    "    'committed_date',\n",
    "    'is_valid',\n",
    "    'geo_location'\n",
    "]\n",
    "column_name_categories_question = [\n",
    "    'question_id',\n",
    "    'question',\n",
    "    'categories_id'\n",
    "]\n",
    "\n",
    "column_name_categories = [\n",
    "    'categories_id',\n",
    "    'name'\n",
    "]\n",
    "\n",
    "column_name_emmission = [\n",
    "    'emission_id',\n",
    "    'facility_id',\n",
    "    'electricity_production_mwh',\n",
    "    'scope_1_emission',\n",
    "    'scope_2_emission',\n",
    "    'emission_intensity',\n",
    "    'grid',\n",
    "]\n",
    "\n",
    "column_state_industry_data = [\n",
    "    'state_data_id',\n",
    "    'label_code',\n",
    "    'label',\n",
    "    'state_code'\n",
    "]\n",
    "\n",
    "column_list_answer_data = [\n",
    "    'list_answer_id',\n",
    "    'answer',\n",
    "    'question_id',\n",
    "    'state_data_id',\n",
    "    'year'\n",
    "]\n",
    "\n",
    "df_states = pd.DataFrame(list(state_abbrev.items()), columns=[\"state_code\", \"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b1d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing SQL:\n",
      " CREATE TABLE IF NOT EXISTS states (\n",
      "    \"state_code\" VARCHAR,\n",
      "    \"name\" VARCHAR,\n",
      "    PRIMARY KEY(\"state_code\")\n",
      ");\n",
      "Table 'states' created successfully.\n",
      "Executing SQL:\n",
      " CREATE TABLE IF NOT EXISTS facility (\n",
      "    \"facility_id\" INTEGER DEFAULT nextval('facility_facility_id_seq'),\n",
      "    \"state_code\" VARCHAR NOT NULL,\n",
      "    \"facility_name\" VARCHAR,\n",
      "    \"postcode\" VARCHAR,\n",
      "    \"source\" VARCHAR,\n",
      "    PRIMARY KEY(\"facility_id\"),\n",
      "    FOREIGN KEY(\"state_code\") REFERENCES states(\"state_code\")\n",
      ");\n",
      "Table 'facility' created successfully.\n",
      "Executing SQL:\n",
      " CREATE TABLE IF NOT EXISTS facility_status (\n",
      "    \"facility_status_id\" INTEGER DEFAULT nextval('facility_status_facility_status_id_seq'),\n",
      "    \"facility_id\" INTEGER NOT NULL,\n",
      "    \"accreditation_code\" VARCHAR,\n",
      "    \"capacity_mw\" DOUBLE,\n",
      "    \"accreditation_start_date\" DATE,\n",
      "    \"approval_date\" DATE,\n",
      "    \"committed_date\" DATE,\n",
      "    \"is_valid\" VARCHAR,\n",
      "    \"geo_location\" VARCHAR,\n",
      "    PRIMARY KEY(\"facility_status_id\"),\n",
      "    FOREIGN KEY(\"facility_id\") REFERENCES facility(\"facility_id\")\n",
      ");\n",
      "Table 'facility_status' created successfully.\n",
      "Executing SQL:\n",
      " CREATE TABLE IF NOT EXISTS emission_data (\n",
      "    \"emission_id\" INTEGER DEFAULT nextval('emission_data_emission_id_seq'),\n",
      "    \"facility_id\" INTEGER NOT NULL,\n",
      "    \"electricity_production_mwh\" DOUBLE,\n",
      "    \"scope_1_emission\" DOUBLE,\n",
      "    \"scope_2_emission\" DOUBLE,\n",
      "    \"emission_intensity\" DOUBLE,\n",
      "    \"grid\" VARCHAR,\n",
      "    PRIMARY KEY(\"emission_id\"),\n",
      "    FOREIGN KEY(\"facility_id\") REFERENCES facility(\"facility_id\")\n",
      ");\n",
      "Table 'emission_data' created successfully.\n",
      "Executing SQL:\n",
      " CREATE TABLE IF NOT EXISTS question (\n",
      "    \"question_id\" INTEGER DEFAULT nextval('question_question_id_seq'),\n",
      "    \"question\" VARCHAR,\n",
      "    \"categories_id\" VARCHAR,\n",
      "    PRIMARY KEY(\"question_id\")\n",
      ");\n",
      "Table 'question' created successfully.\n",
      "Executing SQL:\n",
      " CREATE TABLE IF NOT EXISTS state_industry_data (\n",
      "    \"state_data_id\" INTEGER DEFAULT nextval('state_industry_data_state_data_id_seq'),\n",
      "    \"label_code\" VARCHAR,\n",
      "    \"label\" VARCHAR,\n",
      "    \"state_code\" VARCHAR NOT NULL,\n",
      "    PRIMARY KEY(\"state_data_id\"),\n",
      "    FOREIGN KEY(\"state_code\") REFERENCES states(\"state_code\")\n",
      ");\n",
      "Table 'state_industry_data' created successfully.\n",
      "Executing SQL:\n",
      " CREATE TABLE IF NOT EXISTS list_answer_data (\n",
      "    \"list_answer_id\" INTEGER DEFAULT nextval('list_answer_data_list_answer_id_seq'),\n",
      "    \"answer\" DOUBLE,\n",
      "    \"question_id\" INTEGER NOT NULL,\n",
      "    \"state_data_id\" INTEGER NOT NULL,\n",
      "    \"year\" INTEGER,\n",
      "    PRIMARY KEY(\"list_answer_id\"),\n",
      "    FOREIGN KEY(\"question_id\") REFERENCES question(\"question_id\"),\n",
      "    FOREIGN KEY(\"state_data_id\") REFERENCES state_industry_data(\"state_data_id\")\n",
      ");\n",
      "Table 'list_answer_data' created successfully.\n",
      "Inserted 10 rows into states\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ----------------------------------- CREATE TABLE PROCESS ---------------------------- #\n",
    "\n",
    "# Create states table\n",
    "database.create_table(df = df_states, table_name=\"states\", primary_key=\"state_code\", foreign_keys=None)\n",
    "\n",
    "# create facility table\n",
    "database.create_table(df = pd.DataFrame(columns=column_name_facility), table_name=\"facility\", primary_key=\"facility_id\", auto_increment=True, foreign_keys={\"state_code\": (\"states\", \"state_code\")} )\n",
    "\n",
    "# create facility status table\n",
    "database.create_table(df= pd.DataFrame(columns=column_name_facility_status), auto_increment= True, table_name=\"facility_status\", primary_key=\"facility_status_id\", \n",
    "                      foreign_keys= {\"facility_id\": (\"facility\", \"facility_id\")} ,column_types = {\n",
    "                                                                                                    'facility_status_id': 'INTEGER',\n",
    "                                                                                                    'facility_id': 'INTEGER',\n",
    "                                                                                                    'accreditation_code': 'VARCHAR',\n",
    "                                                                                                    'capacity_mw': 'DOUBLE',\n",
    "                                                                                                    'accreditation_start_date': 'DATE',\n",
    "                                                                                                    'is_valid': 'VARCHAR',\n",
    "                                                                                                    'approval_date': 'DATE',\n",
    "                                                                                                    'committed_date': 'DATE'\n",
    "                                                                                                }\n",
    "                        )\n",
    "# create emission detail table\n",
    "database.create_table(df = pd.DataFrame(columns=column_name_emmission), table_name = \"emission_data\", primary_key = \"emission_id\", auto_increment=True,\n",
    "                      foreign_keys={\"facility_id\": (\"facility\", \"facility_id\")}, column_types = {\n",
    "                                                                                                    'emission_id': 'INTEGER',             \n",
    "                                                                                                    'facility_id': 'INTEGER',\n",
    "                                                                                                    'electricity_production_mwh':'DOUBLE',             \n",
    "                                                                                                    'scope_1_emission': 'DOUBLE',         \n",
    "                                                                                                    'scope_2_emission': 'DOUBLE',         \n",
    "                                                                                                    'emission_intensity': 'DOUBLE',       \n",
    "                                                                                                    'grid': 'VARCHAR',                  \n",
    "                                                                                                    'geo_location': 'POINT'               \n",
    "                                                                                                }\n",
    "                        )\n",
    "\n",
    "\n",
    "# create question table\n",
    "database.create_table(df = pd.DataFrame(columns=column_name_categories_question), table_name = \"question\", auto_increment=True, primary_key=\"question_id\", column_types={\n",
    "                                                                                                    'question_id': 'INTEGER',   \n",
    "                                                                                                    'question': 'VARCHAR'\n",
    "                                                                                                    }\n",
    "                      )\n",
    "\n",
    "# create state industry data from abs\n",
    "database.create_table(df = pd.DataFrame(columns=column_state_industry_data), table_name=\"state_industry_data\", primary_key=\"state_data_id\", auto_increment=True,\n",
    "                      foreign_keys={\"state_code\":(\"states\", \"state_code\")}, column_types = {\n",
    "                                                                                            'state_data_id': 'INTEGER',\n",
    "                                                                                            'label_code': 'VARCHAR',\n",
    "                                                                                            'label': 'VARCHAR',\n",
    "                                                                                            'state_code': 'VARCHAR'\n",
    "                                                                                        }\n",
    "                      )\n",
    "\n",
    "# create list_answer_data\n",
    "database.create_table(df = pd.DataFrame(columns = column_list_answer_data), auto_increment=True, table_name=\"list_answer_data\", primary_key='list_answer_id',\n",
    "                      foreign_keys={\"question_id\":(\"question\",\"question_id\"), \"state_data_id\":(\"state_industry_data\",\"state_data_id\")},\n",
    "                      column_types={\n",
    "                            'list_answer_id':'INTEGER',\n",
    "                            'answer': 'DOUBLE',\n",
    "                            'question_id': 'INTEGER',\n",
    "                            'state_data_id': 'INTEGER',\n",
    "                            'year':'INTEGER'\n",
    "                      })\n",
    "# ---------------------------------- INSERT DATA PROCESS ------------------------------- # \n",
    "# ---------------------------------- INSERT STATE DATA --------------------------------- #\n",
    "database.insert_data(df = df_states, table_name=\"states\", column_map={\"state_code\":\"state_code\", \"name\":\"name\"})\n",
    "\n",
    "# ---------------------------------- INSERT APPROVED DATA --------------------------------- #\n",
    "insert_tables_facility_status_approved = [\n",
    "    {\n",
    "    \"name\": \"facility\",\n",
    "    \"primary_key\": \"facility_id\",\n",
    "    \"auto_increment\": True,\n",
    "    \"dataset_identifier\": \"power_station_name_clean\", \n",
    "    \"column_map\": {\n",
    "        \"power_station_name_clean\": \"facility_name\",\n",
    "        \"State\": \"state_code\",\n",
    "        \"Postcode\": \"postcode\",\n",
    "        \"Fuel Source (s)\": \"source\"\n",
    "    }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"facility_status\",\n",
    "        \"primary_key\": \"facility_status_id\",\n",
    "        \"auto_increment\": True,\n",
    "        \"foreign_keys\": {\n",
    "            \"facility_id\": (\"facility\", \"facility_id\",\"power_station_name_clean\")\n",
    "        },\n",
    "        \"column_map\": {\n",
    "            \"facility_id\": \"facility_id\",\n",
    "            \"Installed capacity (MW)\": \"capacity_mw\",\n",
    "            \"Accreditation code\": \"accreditation_code\",\n",
    "            \"Approval date\": \"approval_date\",\n",
    "            \"Accreditation start date\": \"accreditation_start_date\",\n",
    "            \"geo_location\": \"geo_location\",\n",
    "            \"valid_flag\": \"is_valid\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# ---------------------------------- INSERT COMMITTED DATA --------------------------------- #\n",
    "\n",
    "insert_tables_facility_status_commited = [\n",
    "    {\n",
    "    \"name\": \"facility\",\n",
    "    \"primary_key\": \"facility_id\",\n",
    "    \"auto_increment\": True,\n",
    "    \"dataset_identifier\": \"Project Name\", \n",
    "    \"column_map\": {\n",
    "        \"Project Name\": \"facility_name\",\n",
    "        \"State \": \"state_code\",\n",
    "        \"Fuel Source\": \"source\"\n",
    "    }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"facility_status\",\n",
    "        \"primary_key\": \"facility_status_id\",\n",
    "        \"auto_increment\": True,\n",
    "        \"foreign_keys\": {\n",
    "            \"facility_id\": (\"facility\", \"facility_id\", \"Project Name\")\n",
    "        },\n",
    "        \"column_map\": {\n",
    "            \"facility_id\": \"facility_id\",\n",
    "            \"MW Capacity\": \"capacity_mw\",\n",
    "            \"committed_date\": \"committed_date\",\n",
    "            \"geo_location\": \"geo_location\",\n",
    "            \"valid_flag\": \"is_valid\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "# ---------------------------------- INSERT PROBABLE DATA --------------------------------- #\n",
    "\n",
    "insert_tables_facility_status_probable = [\n",
    "    {\n",
    "    \"name\": \"facility\",\n",
    "    \"primary_key\": \"facility_id\",\n",
    "    \"auto_increment\": True,\n",
    "    \"dataset_identifier\": \"Project Name\", \n",
    "    \"column_map\": {\n",
    "        \"Project Name\": \"facility_name\",\n",
    "        \"State \": \"state_code\",\n",
    "        \"Fuel Source\": \"source\"\n",
    "    }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"facility_status\",\n",
    "        \"primary_key\": \"facility_status_id\",\n",
    "        \"auto_increment\": True,\n",
    "        \"foreign_keys\": {\n",
    "            \"facility_id\": (\"facility\", \"facility_id\", \"Project Name\")\n",
    "        },\n",
    "        \"column_map\": {\n",
    "            \"facility_id\": \"facility_id\",\n",
    "            \"MW Capacity\": \"capacity_mw\",\n",
    "            \"geo_location\": \"geo_location\",\n",
    "            \"valid_flag\": \"is_valid\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# ---------------------------------- INSERT NGER DATA --------------------------------- #\n",
    "insert_tables_nger = [\n",
    "    {\n",
    "    \"name\": \"facility\",\n",
    "    \"primary_key\": \"facility_id\",\n",
    "    \"auto_increment\": True,\n",
    "    \"dataset_identifier\": \"Facility name\", \n",
    "    \"column_map\": {\n",
    "        \"Facility name\": \"facility_name\",\n",
    "        \"State\": \"state_code\",\n",
    "        \"Primary fuel\":\"source\"\n",
    "    }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"emission_data\",\n",
    "        \"primary_key\": \"emission_id\",\n",
    "        \"auto_increment\": True,\n",
    "        \"foreign_keys\": {\n",
    "            \"facility_id\": (\"facility\", \"facility_id\", \"Facility name\")\n",
    "        },\n",
    "        \"column_map\": {\n",
    "            \"facility_id\": \"facility_id\",\n",
    "            \"Electricity production MWh\": \"electricity_production_mwh\",\n",
    "            \"Total scope 1 emissions t CO2 e\": \"scope_1_emission\",\n",
    "            \"Total scope 2 emissions t CO2 e\": \"scope_2_emission\",\n",
    "            \"Emission intensity t CO2 e MWh\" : \"emission_intensity\",\n",
    "            \"Grid\":\"grid\",\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a1a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ABS dataset inserted successfully using auto-increment IDs.\n"
     ]
    }
   ],
   "source": [
    "database.insert_dataframe_multi_table_rowwise(df=df_geo_approved, tables=insert_tables_facility_status_approved)\n",
    "database.insert_dataframe_multi_table_rowwise(df=df_geo_commited, tables=insert_tables_facility_status_commited)\n",
    "database.insert_dataframe_multi_table_rowwise(df=df_geo_probable, tables=insert_tables_facility_status_probable)\n",
    "database.insert_dataframe_multi_table_rowwise(df=df_nger_cleaning, tables=insert_tables_nger)\n",
    "database.insert_dataframe_bulk_abs_autoinc(lga_area)\n",
    "database.insert_dataframe_bulk_abs_autoinc(statistical_area)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
