{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9c416354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import mimetypes\n",
    "import numpy as np\n",
    "\n",
    "# plot\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# selenium method\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pyperclip\n",
    "import time\n",
    "\n",
    "# analysis\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# duckdb\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db761c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c80fb14",
   "metadata": {},
   "source": [
    "##### Class Download File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cdd4b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- Function List ----------------------------------------\n",
    "\n",
    "\n",
    "class download_file:\n",
    "    def status_bar_api(self, api_url, csv_filename = None, status = 'api'):\n",
    "        # Stream the download\n",
    "        if status == 'api':\n",
    "            with requests.get(api_url, stream=True) as response:\n",
    "                response.raise_for_status()\n",
    "                total_size = int(response.headers.get('content-length', 0))\n",
    "                chunk_size = 1024 * 1024  # 1 MB chunks\n",
    "                \n",
    "                chunks = []\n",
    "                with tqdm(total=total_size, unit='B', unit_scale=True, desc='Downloading') as pbar:\n",
    "                    for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                        if chunk:\n",
    "                            chunks.append(chunk)\n",
    "                            pbar.update(len(chunk))\n",
    "                \n",
    "                # Combine chunks into a single bytes object\n",
    "                content = b''.join(chunks)\n",
    "            \n",
    "            # Save CSV if filename is provided\n",
    "            if csv_filename:\n",
    "                with open(csv_filename, 'wb') as f:\n",
    "                    f.write(content)\n",
    "        else:\n",
    "            response = requests.get(api_url, stream=True)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Determine filename\n",
    "            if \"Content-Disposition\" in response.headers:\n",
    "                content_disposition = response.headers[\"Content-Disposition\"]\n",
    "                filename = content_disposition.split(\"filename=\")[-1].strip('\"')\n",
    "            else:\n",
    "                filename = os.path.basename(api_url)\n",
    "\n",
    "            # If filename has no extension, try to guess from Content-Type\n",
    "            if \".\" not in filename:\n",
    "                content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "                extension = mimetypes.guess_extension(content_type.split(\";\")[0].strip())\n",
    "                if extension:\n",
    "                    filename += extension\n",
    "\n",
    "            # Get total file size for progress bar (in bytes)\n",
    "            total_size = int(response.headers.get(\"content-length\", 0))\n",
    "            chunk_size = 8192  # 8 KB per chunk\n",
    "\n",
    "            # Download with progress bar\n",
    "            with open(filename, \"wb\") as f, tqdm(\n",
    "                total=total_size, unit='B', unit_scale=True, desc=filename\n",
    "            ) as progress_bar:\n",
    "                for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "                \n",
    "\n",
    "    # download from cer \n",
    "    def download_from_cer(self, url, csv_filename=None):\n",
    "        \"\"\"Extract API URL from CER datasets using Selenium and download CSV.\"\"\"\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "\n",
    "        # Find and click \"Copy API URL\" button\n",
    "        button = wait.until(\n",
    "            EC.element_to_be_clickable(\n",
    "                (By.XPATH, \"//button[.//span[contains(text(), 'Copy API URL')]]\")\n",
    "            )\n",
    "        )\n",
    "        button.click()\n",
    "        time.sleep(1)  # wait for clipboard update\n",
    "\n",
    "        api_url = pyperclip.paste()\n",
    "        driver.quit()\n",
    "\n",
    "        cer_code = url.split(\"/\")[-1]\n",
    "        api_url = f\"https://api.cer.gov.au/datahub-public/v1/api/Dataset/NGER/dataset/{cer_code}.csv\"\n",
    "        print(\"Downloading from:\", api_url)\n",
    "\n",
    "        csv_filename = f\"{cer_code}.csv\"\n",
    "        return self.status_bar_api(api_url=api_url, csv_filename=csv_filename)\n",
    "\n",
    "    def download_cer_markets(self, url):\n",
    "        url_header = url.split('/')[2]\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            # Step 2: Find the button/link with the XLSX file\n",
    "        div_tags = soup.find_all(\"div\", class_=\"cer-accordion__body__item\")\n",
    "        for div in div_tags:\n",
    "            a_tag = div.find(\"a\", href=True)\n",
    "            if a_tag:\n",
    "                text = a_tag.get_text(strip=True).lower()  # normalize text\n",
    "                if \"csv\" in text:\n",
    "                    file_href = a_tag[\"href\"]\n",
    "                    full_url = f\"https://www.{url_header}{file_href}\"\n",
    "                    #print(full_url)\n",
    "                    self.status_bar_api(api_url=full_url,csv_filename=None, status='file')\n",
    "    \n",
    "    def download_abs(self, url):\n",
    "        url_header = url.split('/')[2]\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find the link for \"Population and people\"\n",
    "        target_div = None\n",
    "        for div in soup.find_all(\"div\", class_=\"file-description-link-formatter\"):\n",
    "            h4 = div.find(\"h4\")\n",
    "            if h4 and \"Economy and industry\" in h4.text:\n",
    "                target_div = div\n",
    "                break\n",
    "        if target_div:\n",
    "            a_tag = target_div.find(\"a\", href=True)\n",
    "            relative_url = a_tag['href']\n",
    "            download_url = f\"https://{url_header}\" + relative_url\n",
    "            print(\"Found download URL:\", download_url)\n",
    "            self.status_bar_api(api_url=download_url,csv_filename=None, status='file')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513799fb",
   "metadata": {},
   "source": [
    "<h1>Retrieve Data Set </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7f48d9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Download First Dataset ....\n",
      "Downloading from: https://api.cer.gov.au/datahub-public/v1/api/Dataset/NGER/dataset/ID0243.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 83.3k/83.3k [00:00<00:00, 12.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Download Second Dataset ....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "total-lgcs-and-capacity-accredited-power-stations-2025.csv: 100%|██████████| 421/421 [00:00<00:00, 2.98MB/s]\n",
      "power-stations-and-projects-accredited.csv: 100%|██████████| 25.8k/25.8k [00:00<00:00, 25.8MB/s]\n",
      "power-stations-and-projects-committed.csv: 100%|██████████| 1.89k/1.89k [00:00<00:00, 7.78MB/s]\n",
      "power-stations-and-projects-probable.csv: 100%|██████████| 2.22k/2.22k [00:00<00:00, 8.34MB/s]\n",
      "total-lgcs-rec-registry-0.csv: 100%|██████████| 45.4k/45.4k [00:00<00:00, 9.14MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Download Third Dataset ....\n",
      "Found download URL: https://www.abs.gov.au/methodologies/data-region-methodology/2011-24/14100DO0003_2011-24.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14100DO0003_2011-24.xlsx: 100%|██████████| 19.7M/19.7M [00:03<00:00, 5.82MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "download = download_file()\n",
    "\n",
    "# first dataset\n",
    "print(\"Processing Download First Dataset ....\")\n",
    "cer_url = \"https://data.cer.gov.au/datasets/NGER/ID0243\"\n",
    "download.download_from_cer(url=cer_url)\n",
    "# # second dataset\n",
    "print(\"Processing Download Second Dataset ....\")\n",
    "cer_markets_url = \"https://cer.gov.au/markets/reports-and-data/large-scale-renewable-energy-data\"\n",
    "download.download_cer_markets(url=cer_markets_url)\n",
    "# third dataset\n",
    "print(\"Processing Download Third Dataset ....\")\n",
    "abs_url = \"https://www.abs.gov.au/methodologies/data-region-methodology/2011-24#data-downloads\"\n",
    "download.download_abs(url=abs_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469afbad",
   "metadata": {},
   "source": [
    "<h1>Data Integration and Cleaning Data </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92d2a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cleaningData:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def show_null_data(self, data):\n",
    "        null_count = data.isna().sum()\n",
    "        null_percent = data.isna().sum() / len(data)\n",
    "        null_percent = null_percent.apply(lambda x: f\"{x:.1%}\")\n",
    "        print(\"Total Duplicated Count: \", data.duplicated().sum())\n",
    "        results = pd.concat([null_count, null_percent], axis=1)\n",
    "        results.columns = ['Null Total Count', 'Null Percentage']\n",
    "        return results\n",
    "    \n",
    "    def split_categorical_numerical(self, data):\n",
    "        numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "        categorical_cols = data.select_dtypes(include=['object', 'category']).columns\n",
    "        \n",
    "        return numerical_cols, categorical_cols\n",
    "\n",
    "class connectDB:\n",
    "    def __init__(self):\n",
    "        self.connection =  duckdb.connect('my_database.duckdb') \n",
    "    \n",
    "    def create_new_table(self):\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6151a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first url file\n",
    "df_nger = pd.read_csv(\"ID0243.csv\")\n",
    "\n",
    "# second url file\n",
    "df_cer_1_approved = pd.read_csv(\"power-stations-and-projects-accredited.csv\")\n",
    "df_cer_1_commited = pd.read_csv(\"power-stations-and-projects-committed.csv\")\n",
    "df_cer_1_probable = pd.read_csv(\"power-stations-and-projects-probable.csv\")\n",
    "\n",
    "\n",
    "# third url file\n",
    "df_abs = pd.ExcelFile(\"14100DO0003_2011-24.xlsx\")\n",
    "statistical_area = pd.read_excel(df_abs, 'Table 1', header=[5,6])\n",
    "lga_area = pd.read_excel(df_abs, 'Table 2', header=[5,6])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d12e21",
   "metadata": {},
   "source": [
    "#### Cleaning the NGER.CSV dataset and put it into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b26fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Duplicated Count:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2h/b49chrcn56z4tf_5y0zw27740000gn/T/ipykernel_79755/836381434.py:7: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Unknown' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_nger_cleaning.fillna(\"Unknown\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "cleaning_data = cleaningData()\n",
    "\n",
    "df_nger_numerical, df_nger_categorical = cleaning_data.split_categorical_numerical(df_nger) \n",
    "df_nger.replace('-', np.nan, inplace=True)\n",
    "\n",
    "df_nger_cleaning = df_nger.drop(columns=['Electricity production GJ', 'Total emissions t CO2 e',''])\n",
    "df_nger_cleaning.fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "cleaning_data.show_null_data(df_nger_cleaning)\n",
    "\n",
    "\n",
    "# Automatically read CSV and create a table\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE my_table AS\n",
    "SELECT\n",
    "    row_number() OVER () AS id,\n",
    "    *\n",
    "FROM read_csv_auto('ID0243.csv')\n",
    "\"\"\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "387a3f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n"
     ]
    },
    {
     "ename": "ConnectionException",
     "evalue": "Connection Error: Connection already closed!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m df_cer_1_approved\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43mCREATE TABLE power_stations AS\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43mSELECT\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m    row_number() OVER () AS id,\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m    *\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43mFROM read_csv_auto(\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpower-stations-and-projects-accredited.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m con\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mConnectionException\u001b[0m: Connection Error: Connection already closed!"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"----------\")\n",
    "df_cer_1_approved.replace('-', np.nan, inplace=True)\n",
    "\n",
    "con.execute(\"\"\"\n",
    "CREATE TABLE power_stations AS\n",
    "SELECT\n",
    "    row_number() OVER () AS id,\n",
    "    *\n",
    "FROM read_csv_auto('power-stations-and-projects-accredited.csv')\n",
    "\"\"\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96a41ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------\")\n",
    "#show_null_data(df_cer_1_commited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6facd105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------\")\n",
    "#show_null_data(df_cer_1_probable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d483bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------\")\n",
    "#show_null_data(df_cer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabecb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_null_data(df_cer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f11585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"----------\")\n",
    "#show_null_data(statistical_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0c1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------\")\n",
    "#show_null_data(lga_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050c06dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
